# Understanding Docker Images and Containers

Docker images and containers are fundamental to understanding and utilizing Docker effectively. Images serve as the blueprint, while containers are the running instances of those blueprints. This lesson will delve into the intricacies of Docker images and containers, exploring their structure, lifecycle, and relationship to each other. By the end of this lesson, you'll have a solid grasp of how to work with images and containers, setting the stage for building and deploying containerized applications.

## Understanding Docker Images

A Docker image is a read-only template that contains instructions for creating a Docker container. It's like a snapshot of a file system and includes everything needed to run an application: code, runtime, system tools, system libraries, and settings.

### Image Layers

Docker images are built in layers. Each instruction in a Dockerfile (which we'll cover in the next lesson) creates a new layer. These layers are stacked on top of each other to form the final image. This layered architecture offers several advantages:

- **Efficiency:** Layers are cached, so if you rebuild an image and only some layers have changed, Docker only needs to rebuild the changed layers.
- **Sharing:** Multiple images can share common layers, saving disk space.
- **Versioning:** You can revert to previous versions of an image by using a specific layer.

To illustrate, consider a simple example. Let's say you have a base image (e.g., Ubuntu). On top of that, you install Python. Then, you copy your application code. Each of these steps creates a new layer. If you later change your application code, only the application code layer needs to be rebuilt.

### Image Manifest

An image manifest is a JSON file that describes the image, including its layers, size, and architecture. It acts as a table of contents for the image. When you pull an image from a registry, the manifest is downloaded first, allowing Docker to determine which layers are needed.

### Image Registry

A Docker registry is a storage and distribution system for Docker images. The most well-known registry is Docker Hub, a public registry where you can find a vast collection of pre-built images. You can also create your own private registries for storing and sharing images within your organization.

When you use the docker pull command, you're downloading an image from a registry. When you use the docker push command, you're uploading an image to a registry.

### Example: Exploring a Docker Image

Let's consider a practical example using the ubuntu:latest image.

1.  Pull the image:

    ```bash
    docker pull ubuntu:latest
    ```

2.  Inspect the image:
    ```bash
    docker inspect ubuntu:latest
    ```
    This command will output a JSON document containing detailed information about the image, including its layers, environment variables, and entry point. Examine the *RootFS* section to see the layers that make up the image.

### Exercise: Image Exploration

1.  Pull the *nginx:latest* image from Docker Hub.
2.  Use the *docker inspect* command to examine the image.
3.  Identify the layers that make up the image.
4.  What is the *ENTRYPOINT* defined for the *nginx:latest* image?

## Understanding Docker Containers

A Docker container is a runnable instance of a Docker image. It's a lightweight, isolated environment that contains everything needed to run an application. Containers are isolated from each other and from the host operating system, providing a consistent and portable runtime environment.

### Container Lifecycle

A container goes through several stages during its lifecycle:

1.  **Created:** The container is created but not yet running. This happens when you use the *docker create* command (though *docker run* does this implicitly).
2.  **Running:** The container is actively executing its main process. This happens when you use the *docker start* command (or *docker run*, which combines create and start).
3.  **Paused:** The container's processes are frozen. This happens when you use the *docker pause* command.
4.  **Unpaused:** The container's processes are resumed. This happens when you use the *docker unpause* command.
5.  **Stopped:** The container is no longer running, but it still exists. This happens when you use the *docker stop* command.
6.  **Killed:** The container is forcibly stopped. This happens when you use the *docker kill* command.
7.  **Removed:** The container is permanently deleted. This happens when you use the *docker rm* command.

### Container Isolation

Containers are isolated from each other and from the host operating system using kernel features like namespaces and cgroups.

- **Namespaces:** Provide isolation for various system resources, such as process IDs, network interfaces, and mount points. This means that processes inside a container cannot see or interact with processes outside the container, and vice versa.
- **cgroups (Control Groups):** Limit the resources that a container can use, such as CPU, memory, and disk I/O. This prevents one container from monopolizing resources and affecting the performance of other containers or the host system.

### Container Networking

Containers can be connected to networks, allowing them to communicate with each other and with the outside world. Docker provides several networking options, including:

- **Bridge network:** The default network, which allows containers on the same host to communicate with each other.
- **Host network:** The container shares the host's network namespace, giving it direct access to the host's network interfaces.
- **Overlay network:** Allows containers on different hosts to communicate with each other.

### Container Storage

Containers have their own file system, which is isolated from the host operating system. Changes made to the container's file system are not persisted when the container is stopped, unless you use volumes.

- **Volumes:** Provide a way to persist data across container restarts and share data between containers. Volumes can be either host-mounted volumes (where a directory on the host is mounted into the container) or named volumes (where Docker manages the storage location).

### Example: Running a Docker Container

Let's run a simple "hello world" container using the *ubuntu:latest* image.

1.  Run the Container:

    ```bash
    docker run ubuntu:latest echo "Hello, Docker!"
    ```

   This command will create and start a container based on the *ubuntu:latest* image, execute the *echo "Hello, Docker!"* command inside the container, and then stop the container.

2. Run an interactive container:

    ```bash
    docker run -it ubuntu:latest bash
    ```

   This command will create and start a container in interactive mode, giving you a shell prompt inside the container. You can then run commands inside the container, such as *ls*, *pwd*, and *apt-get update*. The -it flags allocate a pseudo-TTY connected to the container and keep STDIN open, even if not attached.

3.  List running containers:

    ```bash
    docker ps
    ```

   This command will show you a list of currently running containers. 

4.  List all containers:

    ```bash
    docker ps -a
    ```

   This command will show you a list of all containers, including those that are stopped.
5.  Stop a container:

    ```bash
    docker stop <container_id>
    ```

   Replace `<container_id>` with the ID of the container you want to stop. You can find the container ID using the `docker ps` or `docker ps -a` commands.
6.  Remove a container:

    ```bash
    docker rm <container_id>
    ```

   Replace `<container_id>` with the ID of the container you want to remove. You must stop a container before you can remove it.

### Exercise: Container Manipulation

1.  Run an *nginx:latest* container in detached mode (using the *-d* flag), mapping port 80 on the host to port 80 on the container (using the *-p* flag).
2.  Use the *docker ps* command to verify that the container is running.
3.  Access the Nginx welcome page in your web browser by navigating to *http://localhost*.
4.  Stop the container using the *docker stop* command.
5.  Remove the container using the *docker rm* command.

## Relationship Between Images and Containers

The relationship between Docker images and containers is analogous to the relationship between classes and objects in object-oriented programming. An image is like a class, defining the blueprint for creating containers. A container is like an object, an instance of an image.

Multiple containers can be created from the same image. Each container is an isolated instance, with its own file system, processes, and network interfaces. Changes made to one container do not affect other containers or the underlying image.

When you run a container, Docker creates a thin, writable layer on top of the image's read-only layers. This writable layer is where changes made by the container are stored. When the container is stopped, this writable layer is discarded (unless you're using volumes).

## Hypothetical Scenario: Containerizing a Web Application

Imagine you're developing a web application using Python and Flask. Your application depends on specific versions of Python libraries and system tools. To ensure that your application runs consistently across different environments (development, testing, production), you can containerize it using Docker.

1.  **Create a Dockerfile:** You'll create a Dockerfile that specifies the base image (e.g., *python:3.9*), installs the necessary Python libraries (using *pip*), copies your application code, and defines the command to start your application (e.g., *python app.py*).
2.  **Build the image:** You'll use the *docker build* command to build a Docker image from your Dockerfile. This image will contain everything needed to run your application.
3.  **Run the container:** You'll use the *docker run* command to create and start a container based on your image. This container will run your web application in an isolated environment.

By containerizing your web application, you can ensure that it runs consistently across different environments, regardless of the underlying infrastructure.

## Summary and Next Steps

In this lesson, we've explored the fundamental concepts of Docker images and containers. You've learned that images are read-only templates used to create containers, and containers are runnable instances of those images. You've also learned about the layered architecture of images, the lifecycle of containers, and the isolation and networking features that Docker provides.

In the next lesson, we'll dive into writing Dockerfiles, which are used to define how Docker images are built. You'll learn how to use Dockerfile instructions to specify the base image, install dependencies, copy application code, and configure the container environment. This will enable you to create custom Docker images tailored to your specific application requirements.
